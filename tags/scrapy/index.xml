<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scrapy on painterliu的博客</title>
    <link>http://painterliu.com/tags/scrapy/index.xml</link>
    <description>Recent content in Scrapy on painterliu的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <atom:link href="http://painterliu.com/tags/scrapy/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>用Scrapy写爬虫</title>
      <link>http://painterliu.com/post/2014-12-07-scrapy/</link>
      <pubDate>Sun, 07 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://painterliu.com/post/2014-12-07-scrapy/</guid>
      <description>

&lt;p&gt;最近懒，没怎么更新博客，赶紧在周末抽出时间写写。&lt;/p&gt;

&lt;p&gt;前一段时间想翻翻&lt;a href=&#34;http://www.freebuf.com/&#34;&gt;freebuf网站&lt;/a&gt;的内容（黑客与画家^_^），一页一页翻太麻烦，本着黑客精神，就想着写个爬虫把网站内容题目爬下来，对感兴趣的题目可以点进去看。&lt;/p&gt;

&lt;p&gt;先声明，原来从来没写过爬虫，于是为了偷懒，找到了Python的爬虫框架&lt;a href=&#34;http://scrapy.org/&#34;&gt;Scrapy&lt;/a&gt;,官方文档看&lt;a href=&#34;https://scrapy-chs.readthedocs.org/zh_CN/latest/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;有框架就是快，突击学习了两天就把爬虫写出来了。&lt;/p&gt;

&lt;h3 id=&#34;scrapy安装&#34;&gt;Scrapy安装&lt;/h3&gt;

&lt;p&gt;自己搜索吧，这里不提。
我用的是Ubuntu系统，刚学会了一个小技巧。安装过程中报错缺少头文件，可试着用一下命令:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install apt-file 
apt-file update 
apt-file search /***  #这里代表提示缺少的文件及路径
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的命令找到相关的包、软件后安装即可，注意会搜索出来很多，一般为后几个字母为lib的包。
###创建项目
选择合适的目录，创建freebuf项目:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrapy startproject freebuftools
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令将创建如下目录文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;freebuftools/
    scrapy.cfg
    freebuftools/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些文件分别是:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;scrapy.cfg: 项目的配置文件&lt;/li&gt;
&lt;li&gt;freebuftools/: 该项目的python模块。之后您将在此加入代码。&lt;/li&gt;
&lt;li&gt;freebuftools/items.py: 项目中的item文件.&lt;/li&gt;
&lt;li&gt;freebuftools/pipelines.py: 项目中的pipelines文件.&lt;/li&gt;
&lt;li&gt;freebuftools/settings.py: 项目的设置文件.&lt;/li&gt;
&lt;li&gt;freebuftools/spiders/: 放置spider代码的目录.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;定义item&#34;&gt;定义Item&lt;/h3&gt;

&lt;p&gt;Item 是保存爬取到的数据的容器；其使用方法和python字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。&lt;/p&gt;

&lt;p&gt;由于我只想获得freebuf里文章的题目和链接，于是只定义了两个相应字段:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import scrapy
class FreebuftoolsItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title=scrapy.Field()#用于保存题目
    link =scrapy.Field()#用于保存链接

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;分析网站结构&#34;&gt;分析网站结构&lt;/h3&gt;

&lt;p&gt;使用浏览器的审查元素功能查看freebuf网站分类浏览下的html代码，可以发现文章目录一条一条的都被包含在&lt;code&gt;&amp;lt;div class=&amp;quot;news_inner news-list&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;&lt;/code&gt;标签里面&lt;/p&gt;

&lt;p&gt;具体结构如下图:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://painterliu.com/img/html.jpg&#34; alt=&#34;文章题目结构&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过目录结构可以提取出文章题目的xpath:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//div[@class=&#39;news_inner news-list&#39;]/div[@class=&#39;news-info&#39;]/dl/dt/a/text()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文章链接的xpath：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//div[@class=&#39;news_inner news-list&#39;]/div[@class=&#39;news-info&#39;]/dl/dt/a/@href
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;光有文章题目和内容还不够，还要知道怎样进入下一页，查看html代码可知下一页的链接格式为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;news-more&amp;quot; id=&amp;quot;pagination&amp;quot;&amp;gt;
    &amp;lt;a href=&amp;quot;http://www.freebuf.com/xxx/page/xxx&amp;quot;&amp;gt;查看更多&amp;lt;/a&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提取出下一页链接xpath:&lt;code&gt;//div[@id=&#39;pagination&#39;]/a/@href&lt;/code&gt;
具体xpath的教程可以查看&lt;a href=&#34;https://scrapy-chs.readthedocs.org/zh_CN/latest/&#34;&gt;scrapy文档&lt;/a&gt;，或者看这里&lt;a href=&#34;http://www.w3school.com.cn/xpath/&#34;&gt;xpath教程&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;有一个小技巧，浏览器中看html代码时可以右键选择&lt;code&gt;复制xpath&lt;/code&gt;，不过复制的路径比较死板。&lt;/p&gt;

&lt;h3 id=&#34;编写spider&#34;&gt;编写spider&lt;/h3&gt;

&lt;p&gt;首先在目录&lt;code&gt;freebuftools\freebuftools\spiders\&lt;/code&gt;下新建文件&lt;code&gt;freebuftools_Spider.py&lt;/code&gt;。
代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#encoding=utf-8
import scrapy
from freebuftools.items import FreebuftoolsItem
class FreeBufToolsSpider(scrapy.Spider):
    name = &amp;quot;freeBufTools&amp;quot;
    allowed_domains =[&amp;quot;freebuf.com&amp;quot;]#域名
    start_urls =[&amp;quot;http://www.freebuf.com/geek&amp;quot;]#初始页面，可更改为相应分类首页
    def parse(self,response):
        file=open(&amp;quot;freebufgeek.html&amp;quot;,&#39;a&#39;)#写入爬取内容的文件，注意文件为追加模式

        for sel in response.xpath(&amp;quot;//div[@class=&#39;news_inner news-list&#39;]/div[@class=&#39;news-info&#39;]/dl/dt/a&amp;quot;):#xpath路径父节点
            tools=FreebuftoolsItem()#新建保存item
            tools[&#39;title&#39;]=sel.xpath(&amp;quot;text()&amp;quot;).extract()[0].encode(&#39;utf-8&#39;)#提取文章题目并转换格式保存
            tools[&#39;link&#39;]=sel.xpath(&amp;quot;@href&amp;quot;).extract()[0].encode(&#39;utf-8&#39;)#提取文章链接并转换格式保存
            html=&#39;&#39;&#39;&amp;lt;p&amp;gt;&amp;lt;a href=&amp;quot;&#39;&#39;&#39;+tools[&#39;link&#39;]+&#39;&#39;&#39;&amp;quot; target=&amp;quot;_black&amp;quot;&amp;gt;&#39;&#39;&#39;+tools[&#39;title&#39;]+&amp;quot;&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;&amp;quot;#拼接写入文件的html代码
            file.writelines(html)#将爬取内容写入文件
            yield tools#返回爬取内容
        url=response.xpath(&amp;quot;//div[@id=&#39;pagination&#39;]/a/@href&amp;quot;).extract()[0]#获取下一页的链接
        yield scrapy.Request(url,callback=self.parse)#回调本函数继续爬取下一页
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码写的比较清楚，内容也很简单，不懂的可以查看官方文档，另外有点需要注意，保存爬取内容的文件打开方式为&lt;strong&gt;追加&lt;/strong&gt;，如果不这样只能保存一条爬取的数据。&lt;/p&gt;

&lt;p&gt;改变初始页面可以爬取不同分类下的文章题目。&lt;/p&gt;

&lt;p&gt;还有Python中yield的用法，简单来说就是yield可在函数中返回内容而仍使函数能继续执行。具体Google百度。&lt;/p&gt;

&lt;h3 id=&#34;执行爬虫&#34;&gt;执行爬虫&lt;/h3&gt;

&lt;p&gt;打开最外层freebuftools文件夹，运行命令&lt;code&gt;scrapy crawl freeBufTools&lt;/code&gt;即可。运行完后可找到自己定义的爬取内容的文件。&lt;/p&gt;

&lt;p&gt;我这里有个问题就是记得创建项目时字母用的小写，但执行时发现小写不对，大家可以在执行前先运行一下&lt;code&gt;scrapy list&lt;/code&gt;查看项目名称。
###遇到的问题
主要是文件的问题，一开始我是想直接通过管道导出json格式文件的，但是发现输出的文件内容是汉字对应的utf-8编码，如图:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://painterliu.com/img/json.PNG&#34; alt=&#34;json输出形式&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后虽然解决了输出汉字的问题但想着json文件还要写个脚本读取，就直接在spider中往文件里写内容了，代码就是上面贴的代码。解决输出内容的方法可以看&lt;a href=&#34;http://git.oschina.net/ldshuang/imax-spider/commit/1d05d7bafdf7758f7b422cc1133abf493bf55086&#34;&gt;这里&lt;/a&gt;，声明:只是用的这里的方法，自己根据具体内容做相应改动。主要&lt;code&gt;pipelines.py&lt;/code&gt;文件和&lt;code&gt;settings.py&lt;/code&gt;文件&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>